# COMP576 Final Project
In this project, we utilized a dataset provided by Vanderbilt
University, which includes student-written essays and corre-
sponding scores. The essays represent the textual content of
student submissions, while the scores, ranging from 1 to 10,
reflect evaluations based on various criteria. This dataset is
fundamental to training and testing our system, ensuring its
accuracy and reliability.

Automated Essay Scoring (AES) is a transformative appli-
cation of natural language processing (NLP) designed to repli-
cate human scoring of written essays with high accuracy and
consistency. The primary objective is to address the challenges
educators face in grading essays—such as time constraints and
subjectivity in evaluation—by creating a system that ensures
fairness and efficiency in assessment.
The development of a robust AES system carries significant
implications for education and beyond. In classrooms, such
a system can standardize grading, reduce the workload of
teachers, and allow them to focus on delivering personalized
feedback to students. Furthermore, AES systems can enhance
the fairness of scoring, especially in high-stakes standardized
testing environments where consistency is critical.
Transformer models, particularly BERT and RoBERTa,
represent state-of-the-art solutions in NLP tasks. Their bidi-
rectional nature allows them to capture both forward and
backward contexts, making them adept at understanding com-
plex essay structures and evaluating features like relevance,
coherence, grammar, and organization [1]. These models’
ability to generalize well across varied data positions them
as ideal candidates for tackling the limitations of traditional
